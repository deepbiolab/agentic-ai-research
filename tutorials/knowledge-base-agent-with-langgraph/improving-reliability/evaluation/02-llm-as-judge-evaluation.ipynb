{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of AI Agent's Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this demo, we’ll implement a framework for evaluating the responses of AI agent. Whether you're building a chatbot, a knowledge assistant, or a task-specific agent, evaluation is key to ensuring trust, relevance, and continuous improvement.\n",
    "\n",
    "### Scenario\n",
    "\n",
    "Suppose you’ve deployed a knowledge-based agent that answers user questions using company documentation. Now, stakeholders want to know:\n",
    "\n",
    "- How accurate are the responses?\n",
    "- Are the answers grounded in the context provided?\n",
    "- Do they follow the expected format?\n",
    "\n",
    "To answer these questions, you need to implement a response evaluation pipeline that scores or classifies agent outputs based on defined criteria — either using another LLM (automatic evaluation) or a manual review process.\n",
    "\n",
    "\n",
    "The workflow should:\n",
    "\n",
    "- A RAG pipeline for information retrieval: Retrieve, augment, and generate answers.\n",
    "- An LLM-based judge for evaluation.\n",
    "- Quality assessment: Evaluate the answers using RAGAS.\n",
    "- Observability: Log performance metrics in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import log_params, log_metrics\n",
    "from typing import List, Dict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Multiple Model Setup**\n",
    "\n",
    "Three models are initialized:\n",
    "\n",
    "- `llm`: a standard OpenAI model used for generating answers.\n",
    "- `llm_judge`: a more powerful model used to evaluate the generated answers.\n",
    "- `embedding`: OpenAI embeddings used for vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# This will evaluate the responses\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **MLflow Experiment Configuration**\n",
    "\n",
    "- An MLflow experiment is created and a run is started with a custom name.\n",
    "- Run metadata, such as model names and embedding models, are logged as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/08 15:36:15 INFO mlflow.tracking.fluent: Experiment with name 'evaluation-demo' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/tim/Devs/Projects/agentic-ai-research/tutorials/knowledge-base-agent-with-langgraph/improving-reliability/evaluation/mlruns/350342372225944296', creation_time=1746689775119, experiment_id='350342372225944296', last_update_time=1746689775119, lifecycle_stage='active', name='evaluation-demo', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"evaluation-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RunInfo: artifact_uri='file:///Users/tim/Devs/Projects/agentic-ai-research/tutorials/knowledge-base-agent-with-langgraph/improving-reliability/evaluation/mlruns/350342372225944296/f0f466819d544d428a798380ffd2c930/artifacts', end_time=None, experiment_id='350342372225944296', lifecycle_stage='active', run_id='f0f466819d544d428a798380ffd2c930', run_name='llm-as-judge', run_uuid='f0f466819d544d428a798380ffd2c930', start_time=1746689803522, status='RUNNING', user_id='tim'>\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"llm-as-judge\") as run:\n",
    "    log_params(\n",
    "        {\n",
    "            \"embeddings_model\":embeddings_fn.model,\n",
    "            \"llm_model\": llm.model_name,\n",
    "            \"llm_judge_model\": llm_judge.model_name,\n",
    "        }\n",
    "    )\n",
    "    print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Run: data=<RunData: metrics={}, params={'embeddings_model': 'text-embedding-3-large',\n",
       " 'llm_judge_model': 'gpt-4o',\n",
       " 'llm_model': 'gpt-4o-mini'}, tags={'mlflow.runName': 'llm-as-judge',\n",
       " 'mlflow.source.name': '/Users/tim/miniforge3/envs/agent/lib/python3.10/site-packages/ipykernel_launcher.py',\n",
       " 'mlflow.source.type': 'LOCAL',\n",
       " 'mlflow.user': 'tim'}>, info=<RunInfo: artifact_uri='file:///Users/tim/Devs/Projects/agentic-ai-research/tutorials/knowledge-base-agent-with-langgraph/improving-reliability/evaluation/mlruns/350342372225944296/f0f466819d544d428a798380ffd2c930/artifacts', end_time=1746689803531, experiment_id='350342372225944296', lifecycle_stage='active', run_id='f0f466819d544d428a798380ffd2c930', run_name='llm-as-judge', run_uuid='f0f466819d544d428a798380ffd2c930', start_time=1746689803522, status='FINISHED', user_id='tim'>, inputs=<RunInputs: dataset_inputs=[]>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_run_id = run.info.run_id\n",
    "mflow_client = mlflow.tracking.MlflowClient()\n",
    "mflow_client.get_run(mlflow_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Document Processing**\n",
    "\n",
    "- A local PDF document is reloaded.\n",
    "- Text is chunked and embedded into a vector store using Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"evaluation-demo\",\n",
    "    embedding_function=embeddings_fn\n",
    ")\n",
    "\n",
    "# Load and process PDF documents\n",
    "file_path = \"the-era-of-experience.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Store document chunks in the vector database\n",
    "_ = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **State Schema Definition**\n",
    "\n",
    "- A new session state class is defined, extending `MessageGraphState`.\n",
    "- It includes fields for:\n",
    "    - `run_id`\n",
    "    - `question`\n",
    "    - `ground_truth`\n",
    "    - `documents` (retrieved context)\n",
    "    - `answer` (LLM response)\n",
    "    - `evaluation` (dictionary of evaluation metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_id(str), ground_truth(str), evaluation(Dict),vquestion(str), documents(List) and answer(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    run_id: str\n",
    "    question: str\n",
    "    ground_truth: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    evaluation: Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **RAG Node Pipeline**\n",
    "\n",
    "Four functional nodes are reused from previous exercises:\n",
    "\n",
    "- `retrieve`: Similarity search on the vector store.\n",
    "- `augment`: Uses a chat prompt with `question` and `context`.\n",
    "- `generate`: Uses the standard LLM to produce the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    return {\"documents\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                \"If you don't know the answer, just say that you don't know. \" \n",
    "                \"Use three sentences maximum and keep the answer concise. \"\n",
    "                \"\\n# Question: \\n-> {question} \"\n",
    "                \"\\n# Context: \\n-> {context} \"\n",
    "                \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    ).to_messages()\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    ai_message = llm.invoke(state[\"messages\"])\n",
    "    return {\"answer\": ai_message.content, \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Evaluation Node**\n",
    "\n",
    "- A new `evaluate_rag` node is created.\n",
    "\n",
    "- It constructs a dataset of:\n",
    "\n",
    "    - `question`, `answer`, `context`, and `ground_truth`. \n",
    "\n",
    "- Uses `llm_judge` and `evaluate()` from RAGAS to score:\n",
    "\n",
    "    - `faithfulness`\n",
    "    - `context_precision`\n",
    "    - `context_recall`\n",
    "    - `answer_relevancy`\n",
    "\n",
    "- Each metric is logged to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    answer = state[\"answer\"]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": [question],\n",
    "            \"answer\": [answer],\n",
    "            \"contexts\": [[doc.page_content for doc in documents]],\n",
    "            \"ground_truth\": [ground_truth]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    evaluation_results = evaluate(\n",
    "        dataset=dataset,\n",
    "        llm=llm_judge\n",
    "    )\n",
    "    print(evaluation_results)\n",
    "\n",
    "    # Log metrics in MLflow\n",
    "    # The evaluation_results output value is a list\n",
    "    # Example: evaluation_results[\"faithfulness\"][0]\n",
    "    with mlflow.start_run(state[\"run_id\"]):\n",
    "        \n",
    "        log_metrics({\n",
    "            \"faithfulness\": evaluation_results[\"faithfulness\"][0],\n",
    "            \"context_precision\": evaluation_results[\"context_precision\"][0],\n",
    "            \"context_recall\": evaluation_results[\"context_recall\"][0],\n",
    "            \"answer_relevancy\": evaluation_results[\"answer_relevancy\"][0]\n",
    "        })\n",
    "\n",
    "    return {\"evaluation\": evaluation_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Workflow Construction**\n",
    "\n",
    "- A `StateGraph` is created with the following nodes and edges:\n",
    "\n",
    "    - `start → retrieve → augment → generate → evaluate_rag → end`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x337c9a140>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"augment\", augment)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"evaluate_rag\", evaluate_rag)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"augment\")\n",
    "workflow.add_edge(\"augment\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"evaluate_rag\")\n",
    "workflow.add_edge(\"evaluate_rag\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph.get_graph().draw_mermaid_png()\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. **Execution and Evaluation**\n",
    "\n",
    "- A test query is submitted:\n",
    "    - *\"What is the key difference between the era of human data and the era of experience?\"*\n",
    "    - A ground truth reference answer is provided.\n",
    "- The pipeline completes and produces evaluation metrics, e.g.:\n",
    "    - `answer_relevancy: 0.9`\n",
    "    - `faithfulness: 0.85`, etc.\n",
    "- MLflow logs both the parameters and evaluation metrics for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [\n",
    "    {\n",
    "        \"question\": \"What is the key difference between the era of human data and the era of experience?\",\n",
    "        \"ground_truth\": \"The key difference is that in the era of human data, AI systems learn mainly by imitating and \" \n",
    "                        \"fine-tuning on large amounts of human-generated data, which limits them to reproducing \" \n",
    "                        \"human knowledge and abilities. In contrast, the era of experience is defined by AI agents \" \n",
    "                        \"learning predominantly from their own interactions with the environment, allowing them to \" \n",
    "                        \"continually improve, adapt, and discover novel strategies beyond what is available in human data.\" \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d57c0148135402ab2cfb6b5db6119c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 1.0000, 'context_precision': 1.0000, 'faithfulness': 1.0000, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "output = graph.invoke(\n",
    "    {\n",
    "        \"question\": reference[0][\"question\"],\n",
    "        \"ground_truth\": reference[0][\"ground_truth\"],\n",
    "        \"run_id\": mlflow_run_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Inspect in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Run: data=<RunData: metrics={'answer_relevancy': 0.9999999999999996,\n",
       " 'context_precision': 0.999999999975,\n",
       " 'context_recall': 1.0,\n",
       " 'faithfulness': 1.0}, params={'embeddings_model': 'text-embedding-3-large',\n",
       " 'llm_judge_model': 'gpt-4o',\n",
       " 'llm_model': 'gpt-4o-mini'}, tags={'mlflow.runName': 'llm-as-judge',\n",
       " 'mlflow.source.name': '/Users/tim/miniforge3/envs/agent/lib/python3.10/site-packages/ipykernel_launcher.py',\n",
       " 'mlflow.source.type': 'LOCAL',\n",
       " 'mlflow.user': 'tim'}>, info=<RunInfo: artifact_uri='file:///Users/tim/Devs/Projects/agentic-ai-research/tutorials/knowledge-base-agent-with-langgraph/improving-reliability/evaluation/mlruns/350342372225944296/f0f466819d544d428a798380ffd2c930/artifacts', end_time=1746690595731, experiment_id='350342372225944296', lifecycle_stage='active', run_id='f0f466819d544d428a798380ffd2c930', run_name='llm-as-judge', run_uuid='f0f466819d544d428a798380ffd2c930', start_time=1746689803522, status='FINISHED', user_id='tim'>, inputs=<RunInputs: dataset_inputs=[]>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mflow_client.get_run(mlflow_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
