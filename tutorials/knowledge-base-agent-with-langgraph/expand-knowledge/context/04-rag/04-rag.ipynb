{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG: Knowledge Base Agent\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook demonstrates how to create a **knowledge base (KB) agent** using **Retrieval-Augmented Generation (RAG)** techniques in LangGraph. \n",
    "\n",
    "- The agent uses vector embeddings to retrieve relevant context from a document and answers user questions by augmenting the prompt with that context.\n",
    "\n",
    "### Scenario\n",
    "\n",
    "Suppose You’re building a support assistant for a researcher that upload a scholar paper. The assistant should be able to:\n",
    "\n",
    "- Search through the documentation\n",
    "- Retrieve the most relevant sections\n",
    "- Provide helpful answers grounded in the retrieved information\n",
    "\n",
    "This approach, aka **Retrieval-Augmented Generation (RAG)** — is a powerful technique for building agents that are accurate, verifiable, and up-to-date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for RAG Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Documents Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-era-of-experience.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"demo\",\n",
    "    embedding_function=embeddings_fn\n",
    ")\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define State Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a State Schema for managing:\n",
    "\n",
    "- User query\n",
    "- Retrieved documents\n",
    "- Generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent should:\n",
    "- fetch relevant document chunks based on the user query\n",
    "- combine the retrieved documents and use them as context\n",
    "- invoke the LLM to generate a response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Retrieve Node\n",
    "\n",
    "- Performs a similarity search using the vector store.\n",
    "- Retrieves documents related to the input question and stores them in state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    return {\"documents\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Augment Node\n",
    "\n",
    "- Uses a `ChatPromptTemplate` with placeholders for `question` and `context`.\n",
    "- Constructs a system message with relevant document excerpts and user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                \"If you don't know the answer, just say that you don't know. \" \n",
    "                \"Use three sentences maximum and keep the answer concise. \"\n",
    "                \"\\n# Question: \\n-> {question} \"\n",
    "                \"\\n# Context: \\n-> {context} \"\n",
    "                \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    ).to_messages()\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Generate Node\n",
    "\n",
    "- Calls the LLM with the constructed prompt to produce an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "def generate(state: State):\n",
    "    ai_message = llm.invoke(state[\"messages\"])\n",
    "    return {\"answer\": ai_message.content, \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"augment\", augment)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"augment\")\n",
    "workflow.add_edge(\"augment\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    {\"question\": \"How many eras for reinforcement learning history?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context suggests that there are at least three distinct eras in the history of reinforcement learning: the era of simulation, the era of human data, and the emerging era of experience. Each era focuses on different aspects and challenges of reinforcement learning. However, the exact number of eras may vary depending on the classification criteria used.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an assistant for question-answering tasks.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. \n",
      "# Question: \n",
      "-> How many eras for reinforcement learning history? \n",
      "# Context: \n",
      "-> Why Now?\n",
      "Learning from experience is not new. Reinforcement learning systems have previously mastered a large\n",
      "number of complex tasks that were represented in a simulator with a clear reward signal (c.f., approximately,\n",
      "the “era of simulation” in Figure 1). For example, RL methods equalled or exceeded human performance\n",
      "5\n",
      "\n",
      "Reinforcement Learning Methods\n",
      "Reinforcement learning (RL) has a rich history that is deeply rooted in autonomous learning, where agents\n",
      "learn for themselves through direct interaction with their environment. Early RL research yielded a suite of\n",
      "powerful concepts and algorithms. For example, temporal difference learning [35] enabled agents to estimate\n",
      "future rewards, leading to breakthroughs such as superhuman performance in backgammon [39]. Exploration\n",
      "techniques, driven by optimism or curiosity, were developed to help agents discover creative new behaviors\n",
      "and avoid getting stuck in suboptimal routines [2]. Methods like the Dyna algorithm enabled agents to\n",
      "build and learn from models of their world, allowing them to plan and reason about future actions [36,\n",
      "29]. Concepts like options and inter/intra-option learning facilitated temporal abstraction, enabling agents to\n",
      "reason over longer timescales and break down complex tasks into manageable sub-goals [38].\n",
      "\n",
      "Figure 1: A sketch chronology of dominant AI paradigms. The y-axis suggests the proportion of the field’s\n",
      "total effort and computation that is focused on RL.\n",
      "through self-play in board games such as backgammon [39], Go [31], chess [32], poker [22, 6] and Strat-\n",
      "ego [26]; video games such as Atari [21], StarCraft II [40], Dota 2 [4] and Gran Turismo [41]; dextrous\n",
      "manipulation tasks such as Rubik’s cube [1]; and resource management tasks such as data center cooling\n",
      "[13]. Furthermore, powerful RL agents such as AlphaZero [33] exhibited impressive and potentially unlim-\n",
      "ited scalability with the size of the neural network, the quantity of interactive experience, and the duration of\n",
      "thinking time. However, agents based on this paradigm did not leap the gap between simulation (closed prob-\n",
      "lems with singular, precisely defined rewards) to reality (open-ended problems with a plurality of seemingly\n",
      "ill-defined rewards).\n",
      "\n",
      "human-centric RL has enabled an unprecedented breadth of behaviours, it has also imposed a new ceiling\n",
      "on the agent’s performance: agents cannot go beyond existing human knowledge. Furthermore, the era of\n",
      "human data has focused predominantly on RL methods that are designed for short episodes of ungrounded,\n",
      "human interaction, and are not suitable for long streams of grounded, autonomous interaction.\n",
      "The era of experience presents an opportunity to revisit and improve classic RL concepts. This era will\n",
      "bring new ways to think about reward functions that are flexibly grounded in observational data. It will\n",
      "revisit value functions and methods to estimate them from long streams with as yet incomplete sequences. It\n",
      "will bring principled yet practical methods for real-world exploration that discover new behaviours that are\n",
      "radically different from human priors. Novel approaches to world models will be developed that capture the \n",
      "# Answer: \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The context suggests that there are at least three distinct eras in the history of reinforcement learning: the era of simulation, the era of human data, and the emerging era of experience. Each era focuses on different aspects and challenges of reinforcement learning. However, the exact number of eras may vary depending on the classification criteria used.\n"
     ]
    }
   ],
   "source": [
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understood how it works, experiment with new things.\n",
    "\n",
    "- Change the embedding model\n",
    "- Change the parameters of RecursiveCharacterTextSplitter(chunk_size and chunk_overlap)\n",
    "- Use your own document\n",
    "- Add More File Types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
